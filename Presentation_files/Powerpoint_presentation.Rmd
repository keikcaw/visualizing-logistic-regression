---
title: "Visualizing logistic regression results for non-technical audiences"
author: "Abby Kaplan and Keiko Cawley"
date: "September 20, 2022"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

class: inverse, center, middle

# Logistic regression review

`r knitr::opts_knit$set(root.dir='..')`

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(lme4)
library(logitnorm)
library(kableExtra)
library(cowplot)
library(grid)
library(gridExtra)
library(patchwork)
library(knitr)
theme_set(theme_bw())
opts_chunk$set(echo = F, message = F, warning = F, error = F, fig.retina = 3,
               fig.align = "center", fig.width = 6, fig.asp = 0.618,
               out.width = "70%")
```

```{css}
.remark-slide-content h1 {
  margin-bottom: 0em;
}
.remark-code {
  font-size: 75% !important;
}
```

---

# Logistic regression: Binary outcomes

- Use logistic regression to model a binary outcome

--

- Examples from higher education:

--

  - Did the student pass the class?

--

  - Did the student enroll for another term?

--

  - Did the student graduate?

---

# The design of logistic regression

- We want to model the probability that the outcome happened

--

- But probabilities are bounded between 0 and 1

--

- Instead, we model the logit of the probability:

$$
\mbox{logit}(p) = \log\left(\begin{array}{c}\frac{p}{1 - p}\end{array}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n
$$

---

class: inverse, center, middle

# What's the problem?

---

layout: true

# Just tell me "the" effect

- Stakeholders often want to know whether something affects outcomes, and by how much

---

--

- But we don't model probabilities directly

$$
\log\left(\begin{array}{c}\frac{p}{1 - p}\end{array}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n
$$

---

- But we don't model probabilities directly

$$
\boxed{\log\left(\begin{array}{c}\frac{p}{1 - p}\end{array}\right)} = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n
$$

--

- We can solve for _p_:

$$
\begin{aligned}
p & = \mbox{logit}^{-1}(\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n) \\ & \\
& = \frac{e^{\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n}}{1 + e^{\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n}}
\end{aligned}
$$

---

layout: true

# "The" effect is nonlinear in _p_

$$
\begin{aligned}
p & = \frac{e^{\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n}}{1 + e^{\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n}}
\end{aligned}
$$

---

--

```{r}
logistic.curve.0.p = ggplot() +
  stat_function(fun = function(x) (exp(x)/(1+(exp(x)))), color = "#009DD8") +
  scale_x_continuous(expression(beta[0]+beta[1]~X[1]~'+'~'...+'~beta[n]~X[n]),
                     limits = c(-6, 6), breaks = seq(-6, 6, 2)) +
  scale_y_continuous("probability (p)", limits = c(0, 1))
logistic.curve.0.p
```

---

```{r}
logistic.curve.1.p = logistic.curve.0.p +
  annotate("segment", x = -3, xend = -2, y = invlogit(-3), yend = invlogit(-3),
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("segment", x = -2, xend = -2, y = invlogit(-3), yend = invlogit(-2),
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("text", x = -3.5, y = 0.3,
           label = str_wrap("A 1-unit change here leads to a small change in probability",
                            20))
logistic.curve.1.p
```

---

```{r}
logistic.curve.2.p = logistic.curve.1.p +
  annotate("segment", x = 0, xend = 1, y = invlogit(0), yend = invlogit(0),
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("segment", x = 1, xend = 1, y = invlogit(0), yend = invlogit(1),
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("text", x = 3, y = 0.6,
           label = str_wrap("A 1-unit change here leads to a large change in probability",
                            20))
logistic.curve.2.p
```

---

layout: false

class: inverse, center, middle

# Sample dataset and model

---

# Datset

```{r fit_model, include = F}
knitr::read_chunk("R_code/fit_model.R")
```

- Our simulated dataset describes students who took Balloon Animal-Making 201 at University Imaginary

--

```{r dataset_summary_table}
table <- data.frame(
  Variable = c("Mac user", "Wear glasses", "Pet type", "Favorite color", "Prior undergraduate GPA", "Height", "Went to tutoring", "Passed"),
  Possible.Responses = c("TRUE/FALSE", "TRUE/FALSE", "dog, cat, fish, none", "blue, red, green, orange", "0.0-4.0", "54-77 inches", "TRUE/FALSE", "TRUE/FALSE"),
  Variable.Type = c("binary", "binary", "categorical", "categorical", "continuous", "continuous", "binary", "binary")
) 
table %>%
  kbl(col.names = gsub("[.]", " ", names(table))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

---

# Dataset

```{r load-data, echo = T}
```

```{r preview_dataset}
df %>%
  head(12) %>%
  kbl() %>%
  kable_styling(font_size = 12)
```

---

# Model

- Dependent variable: did the student pass?

--

- Continuous variables were centered and standardized

```{r prepare-data-continuous, echo = T}
```

--

- Reference levels for categorical variables:

  - Pet type: none

  - Favorite color: blue

```{r prepare-data-categorical, echo = T}
```

---

# Model

```{r model, echo = T}
```

---

# Model

```{r model, echo = T, highlight.output = c(4, 5, 7, 9, 11, 12, 13)}
```

---

# Causality disclaimer

- Some visualizations strongly imply a causal interpretation

--

- It's your responsibility to evaluate whether a causal interpretation is appropriate

--

- If the data doesn't support a causal interpretation, **don't use a visualization that implies one**

---

class: inverse, center, middle

# Visualization option 1:

# Presenting model coefficients

---

# Model coefficients

``` {r get-coefficients, echo = T}
```

---

# Coefficients in a table

```{r}
coefs.df %>%
  dplyr::select(-parameter) %>%
  kbl(col.names = c("Parameter", "Estimate", "Standard error", "z", "p")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                font_size = 14, full_width = F) %>%
  row_spec(0, align = "c")
```

---

# Coefficients in a table

![blinking_meme](blinking_meme.jpg)

---

# Color palette

```{r colors, include = F}
knitr::read_chunk("visual_folder/colors.R")
```

```{r color-palette, echo = T}
```

---

# Change in log-odds

```{r log_odds, include = F}
knitr::read_chunk("visual_folder/log_odds.R")
```

.pull-left[
```{r change-in-log-odds, echo = T, fig.show = "hide"}
```
]

.pull-right[
```{r change_in_log_odds_plot, out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
log.odds.p +
  guides(color = guide_legend(nrow = 3)) +
  theme(legend.position = "bottom") +
  coord_flip(xlim = c(0.9, 11.1), clip = "off")
```
]

---

# Change in log-odds: Pros

.pull-left[
```{r ref.label = "change_in_log_odds_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
```
]

--

.pull-right[
- It's clear which relationships are positive and which are negative

{{content}}
]

--

- The plot has a transparent relationship to the fitted model

---

# Change in log-odds: Cons

.pull-left[
```{r ref.label = "change_in_log_odds_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
```
]

---

# Change in log-odds: Cons

.pull-left[
```{r out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
log.odds.p +
  guides(color = guide_legend(nrow = 3)) +
  theme(legend.position = "bottom") +
  coord_flip(xlim = c(0.9, 11.1), clip = "off") +
  annotate("rect", ymin = -1, ymax = 0.57, xmin = -1.2, xmax = -0.2,
           fill = NA, color = "red", size = 2)
```
]

--

.pull-right[
- The magnitude of effect is in the log-odds scale

{{content}}
]

--

- What is a 0.4 change in the log-odds?

{{content}}

--

- Is the change between 0.4 and 0.8 log-odds "big" or "small"?

{{content}}

--

- You probably don't want to give your audience a tutorial on the inverse logit function

---

# Secret log-odds

.pull-left[
```{r change-in-log-odds-adjusted-axis, echo = T, fig.show = "hide"}
```
]

.pull-right[
```{r change_in_log_odds_adjusted_axis_plot, out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
secret.log.odds.p +
  guides(color = guide_legend(nrow = 3)) +
  theme(legend.position = "bottom") +
  coord_flip(xlim = c(0.9, 11.1), ylim = c(-1.6, 1.5), clip = "off")
```
]

---

# Secret log-odds

.pull-left[
```{r ref.label = "change_in_log_odds_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
```
]

---

# Secret log-odds: Pros

.pull-left[
```{r change_in_log_adds_adjusted_axis_highlighted_plot, out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
secret.log.odds.p +
  guides(color = guide_legend(nrow = 3)) +
  theme(legend.position = "bottom") +
  coord_flip(xlim = c(0.9, 11.1),  ylim = c(-1.6, 1.5), clip = "off") +
  annotate("rect", ymin = -1.45, ymax = 1.45, xmin = -1.2, xmax = 0.5,
           fill = NA, color = "red", size = 2)
```
]

--

.pull-right[
- Easy: just relabel the x-axis

{{content}}
]

--

- No numbers for your audience to misinterpret

---

# Secret log-odds: Cons

.pull-left[
```{r ref.label = "change_in_log_adds_adjusted_axis_highlighted_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3}
```
]

--

.pull-right[
- Can't convey absolute magnitude of an effect

{{content}}
]

--

- Your audience might ask "where are the numbers?" anyway

---

layout: true

#  Change in odds ratio

- Your audience may be more familiar with the "odds" part of log-odds

---

---

$$\log\left(\begin{array}{c}\frac{p}{1 - p}\end{array}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n$$

---

$$\log\left(\boxed{\begin{array}{c}\frac{p}{1 - p}\end{array}}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n$$

--

- Can't we just exponentiate to get the odds?

--

$$\frac{p}{1 - p} = e^{\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n}$$

--

- Now the effect of a coefficient is multiplicative, not additive

--

$$\frac{p}{1 - p} = e^{\beta_ix_i} \cdot e^{\beta_0 + \beta_1x_1 + \ldots + \beta_{i-1}x_{i-1} + \beta_{i+1}x_{i+1} + \ldots + \beta_nx_n}$$

---

layout: false

# Change in odds ratio

```{r odds_ratio, include = F, cache = F}
knitr::read_chunk("visual_folder/odds.R")
```

.pull-left[
```{r odds-ratio-adjusted-axis, echo = T, fig.show = "hide"}
```
]

.pull-right[
```{r odds_ratio_adjusted_axis_plot, out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
odds.ratio.p +
  guides(color = guide_legend(nrow = 3)) +
  theme(legend.position = "bottom")
```
]

---

# Change in odds ratio: Pros

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

--

.pull-right[
- Changes in odds might be easier to describe than changes in log-odds

{{content}}
]

--

- Still pretty easy: a simple transformation of your model coefficients

---

# Change in odds ratio: Cons

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

--

.pull-right[
- Not the way we usually describe odds
]

---

# Change in odds ratio: Cons

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

.pull-right[
- Not the way we usually describe odds

  - Usually use integers: "3-to-1" or "2-to-5", not "3" or "0.4”
]

---

# Change in odds ratio: Cons

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

.pull-right[
- Not the way we usually describe odds

  - Usually use integers: "3-to-1" or "2-to-5", not "3" or "0.4”

  - The unfamiliar format may undo the benefit of using a familiar concept

{{content}}
]

--

- Exponentiated coefficients don't represent odds directly; they represent _changes_ in odds

---

# Change in odds ratio: Cons

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

.pull-right[
- Not the way we usually describe odds

  - Usually use integers: "3-to-1" or "2-to-5", not "3" or "0.4”

  - The unfamiliar format may undo the benefit of using a familiar concept

- Exponentiated coefficients don't represent odds directly; they represent _changes_ in odds

  - Percent change in odds (200% = double the odds) might be misinterpreted as a probability
]

---

# Change in odds ratio: Cons

.pull-left[
```{r ref.label = "odds_ratio_adjusted_axis_plot", out.width = "100%", fig.width = 4.4, fig.asp = 1.3, fig.retina = 6}
```
]

.pull-right[
- Not the way we usually describe odds

  - Usually use integers: "3-to-1" or "2-to-5", not "3" or "0.4”

  - The unfamiliar format may undo the benefit of using a familiar concept

- Exponentiated coefficients don't represent odds directly; they represent _changes_ in odds

  - Percent change in odds (200% = double the odds) might be misinterpreted as a probability

  - Now we're pretty far removed from familiar scales
]

---

class: inverse, center, middle
# Visualization Option 2
# Presenting probabilities

---

# Probabilities relative to some baseline

--

-  Problem with presenting probabilities with logistic regression: the change in percentage points depends on the baseline starting value

--

- We can choose an appropriate baseline probability 

--

- One option for the baseline is to use the overall intercept of the model 

---

# Probabilities relative to some baseline

```{r probability_baseline, include = F}
knitr::read_chunk("visual_folder/probability_baseline.R")
```

```{r probability-relative-to-some-baseline-no-arrows, echo = F, fig.align = 'center'}
```

---

# Probabilities relative to some baseline


```{r probability-relative-to-some-baseline-with-arrows, echo = F, fig.align = 'center'}
```

---

# Probabilities relative to some baseline

--

## Pros

--

- Uses a scale that your audience is already very familiar with: probabilities (expressed as percentages)

--

- Avoids a common, but misleading, way of presenting changes in probabilities: the "percent change" formulation

---

# Probabilities relative to some baseline

--

## Cons

--

- Have to choose a baseline

--

- Representing an "average" student seems reasonable, but the process for doing so depends on how the model is specified:

--

  - the intercept represents a student with average values for the continuous predictors because we standardized those predictors
  
--

  - with respect to the categorical predictors, our baseline represents a student who is _not_ actually all that average: it's a student who is not a Mac user, doesn't wear glasses, etc. All of our baseline categories are the most frequent values in the dataset


---

# Multiple baselines by group

--

Instead of choosing one baseline we can choose several baselines

---

# Multiple baselines by group

--

```{r probability_group, include = F}
knitr::read_chunk("visual_folder/probability_group.R")
```

```{r probability-relative-to-some-baseline-and-group-no-arrows, fig.align = 'center'}
```

---

# Multiple baselines by group

```{r probability-relative-to-some-baseline-and-group-with-arrows, fig.align = 'center'}
```

---

# Multiple baselines by group

--

## Pros

--

- Emphasizes that the baseline we show is a _choice_, and that different students have different baselines

--

- Explicitly shows how the effect of a given predictor varies depending on the baseline

--

- Show different effects by group

--

   - In our example, the larger effect for students with fish is purely a function of their lower baseline

--

  - Imagine a model with true _interactions_ between pet type and other predictors

---

# Multiple baselinse by group

-- 

## Cons

--

- Still have to _choose_ a baseline


--

- More cluttered than a single graph

---

# Banana graphs

--

- We can overcome this baseline picking-and-choosing problem by iterating across every baseline.

--

- For example:

--

  - we can calculate the 0% to 100% predicted probability of passing Balloon Animal-Making 201 for a student who does not own a pet fish.
  
--

  - use those baseline values to compute the predicted probability of a student who does own a pet fish
  
---

# Banana graphs

```{r banana_graphs, include = F}
knitr::read_chunk("visual_folder/banana_graphs.R")
```

```{r create-banana-graph, include = F}
```

```{r banana-graph-bare, fig.align = 'center'}
```

---

# Banana graphs

- **x axis** values for each point on the curve is the baseline probability, i.e., the probability that the student does not own a pet fish

--

- **y axis** values for each point on the curve is the probability increase or decrease from the baseline, i.e., the probability that the student does own a pet fish

--

- **solid line** which goes diagonally across the middle of the graph as a reference line: 

--

  - **Above the solid line**: the predictor variable has a higher predicted probability of passing than its baseline
  
--
  - **On the solid line**: the predictor variable has the same predicted probability of passing as its baseline
  
--

  - **Below the solid line**: the predictor variable has a lower predicted probability of passing than its baseline


---

# Banana graphs

```{r banana-graph-annotated, fig.width = 9, fig.align = 'center'}
```

---

# Banana graphs

```{r banana-graph-multiple,fig.align = 'center'}
```

---

# Banana graphs

--

## Pros

--

- Do not have to pick and choose a baseline &rarr; show the whole range of predicted probabilities

--

## Cons

- Can take up quite a bit of space

--

- Audience's eyes have to dart from one graph to the next to compare any two predictor variables

--

- Initially it could be difficult to understand

---

class: inverse, center, middle
# Visualization Option 3
# Presenting counterfactual counts

---

# Presenting counterfactual counts

--

- Sometimes stakeholders are interested in a measure that's even more basic than probabilities: **the number of times something happens (or doesn't happen)**

--

- Example: suppose your stakeholders want to use your analysis to assess the impact of tutoring on pass rates in Balloon Animal-Making 201

--

  - They're interested not just in _whether_ tutoring helps students, but _how much_ it helps them.
  

---

# Extra successes

## The idea

--

In our dataset, `r format(sum(df$tutoring), big.mark = ",")` students received tutoring; of those, `r format(sum(df$tutoring & df$passed), big.mark = ",")` passed the class. 

--

Suppose those students had _not_ received tutoring; in that case, how many students do we think would have passed? 

--

In other words, how many "extra" passes did we get because of tutoring?

---

# Extra successes

## The approach

--

We can get a simple point estimate by using our model to predict outcomes for the subset of our data where students received tutoring, but with the `tutoring` predictor set to `FALSE` instead of `TRUE` -- in other words, by running a set of counterfactual predictions. 

--

A point estimate doesn't convey the amount of uncertainty around our estimate; we can get confidence intervals by simulating many sets of outcomes and aggregating over them.

---

# Extra successes

```{r counterfactuals, include = F}
knitr::read_chunk("visual_folder/counterfactuals.R")
```

```{r extra-passes, fig.align = 'center'}
```

---

# Extra successes

--

## Pros

--

- This approach is clear and straightforward if your stakeholders care about the magnitude of an effect in terms of counts

--

## Cons

--

- It strongly implies a causal relationship between the predictor and the outcome: we're claiming that the tutoring _caused_ some students to pass who otherwise wouldn't have

--

- Assumes that the counterfactual makes sense

--

- Using a histogram to summarize the simulations: audience might be distracted by the whole idea of using simulation to estimate uncertainty

---

# Extra successes by group

We can summarize counterfactuals by group for a more fine-grained view of our model's predictions
 
---

# Extra successes by group

```{r extra-passes-by-group, fig.align = 'center'}
```

---

# Extra successes by group

--

## Pros

--

-  Avoids a scale that shows the number of simulations, and instead focuses on the range of predictions 

--

- Shows how the effect varies by group

--

## Pro/Con

--

Recall, tutoring gives a larger percentage point boost to students with fish, because those students started out with a lower baseline probability of passing. 

--

But this graph shows that the absolute _number_ of extra passing students with fish is _smaller_ than for other groups; this is because there simply aren't that many students with fish in the first place.

---

# Potential successes compared to group size

--

- Attempt to show _both_ the effect size for each group _and_ the overall size of that group

--

- Switch the direction of the counterfactual: instead of predicting how many tutored students passed who otherwise wouldn't have, we're going to predict how many _untutored_ students _would have_ passed if they had received tutoring

---

# Potential successes compared to group size

```{r potential-passes-by-group, fig.align = 'center'}
```

---

# Potential successes compared to group size

--

## Pro

- Step towards acknowledging different group sizes, and it also helps put the absolute numbers in context

--

## Cons

--

- Makes a strong causal claim, which may not be appropriate for your model

--

- Doesn't convey the size of the _whole_ group, only of the number of passing students in the group 

--

- If you have groups of vastly unequal sizes, then the effects for smaller groups will be squashed at the bottom of the scale and difficult to see

---

# Concluding Remarks

--

- There is no right or wrong way, only better and worse ways, so, get creative! 

--

- Knowing your stakeholders as well as the context and purpose of your research should be your guides to determine which visualization is most appropriate

--

- Use colors, the layout, and annotations to your advantage

--

- Share your ideas with others

---

class: inverse, center, middle
# Thank you


